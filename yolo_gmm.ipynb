{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e721c352-83f8-4643-be07-e7c54457ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import YOLOv3\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669d88f-3992-4787-b307-bc8244c4728c",
   "metadata": {},
   "source": [
    "# yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2c1eb0cb-e173-4c1e-8c6d-12503da2765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b3998c52-9d35-458c-b04c-ad906cde0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Information about architecture config:\n",
    "Tuple is structured by (filters, kernel_size, stride) \n",
    "Every conv is a same convolution. \n",
    "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
    "\"S\" is for scale prediction block and computing the yolo loss\n",
    "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
    "\"\"\"\n",
    "yolo_config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],  # To this point is Darknet-53\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "# SSD300 CONFIGS for COCO (active learning)\n",
    "coco300_active = {\n",
    "    'num_classes': 81,\n",
    "    'lr_steps': (80000, 100000, 120000),\n",
    "    'max_iter': 120000,\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'min_dim': 300,\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [21, 45, 99, 153, 207, 261],\n",
    "    'max_sizes': [45, 99, 153, 207, 261, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': True,\n",
    "    'num_initial_labeled_set': 5000,\n",
    "    'num_total_images': 82081,\n",
    "    'acquisition_budget': 1000,\n",
    "    'num_cycles': 3,\n",
    "    'name': 'COCO',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "88c59150-1103-417e-9607-ad1de8606703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=80):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []  # for each scale\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2),)\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers\n",
    "'''\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31f449-974f-4381-be63-a79472735bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a17811ed-c8d0-4fea-b597-487870236939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for testing\n",
    "# model = YOLOv3(num_classes=80)\n",
    "# x = torch.randn((2, 3, 416, 416))\n",
    "# out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "21066182-da6e-4f49-a53c-e55fd758b1ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4201db2-d3c8-4549-8c88-87c04182f7f5",
   "metadata": {},
   "source": [
    "# yolo gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428a979-3e84-4f26-867d-823fc31382f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a9d36-f795-469a-a72e-416ec1940170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "352f1389-3639-434d-bd9b-9b745ab388be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multibox(out_channels, num_classes):\n",
    "    number_anchor = 3\n",
    "    \n",
    "    loc_mu_1_layers = []\n",
    "    loc_var_1_layers = []\n",
    "    loc_pi_1_layers = []\n",
    "    loc_mu_2_layers = []\n",
    "    loc_var_2_layers = []\n",
    "    loc_pi_2_layers = []\n",
    "    loc_mu_3_layers = []\n",
    "    loc_var_3_layers = []\n",
    "    loc_pi_3_layers = []\n",
    "    loc_mu_4_layers = []\n",
    "    loc_var_4_layers = []\n",
    "    loc_pi_4_layers = []\n",
    "    conf_mu_1_layers = []\n",
    "    conf_var_1_layers = []\n",
    "    conf_pi_1_layers = []\n",
    "    conf_mu_2_layers = []\n",
    "    conf_var_2_layers = []\n",
    "    conf_pi_2_layers = []\n",
    "    conf_mu_3_layers = []\n",
    "    conf_var_3_layers = []\n",
    "    conf_pi_3_layers = []\n",
    "    conf_mu_4_layers = []\n",
    "    conf_var_4_layers = []\n",
    "    conf_pi_4_layers = []\n",
    "\n",
    "    for c in out_channels:\n",
    "        # for loc and conf (mu var pi) \n",
    "        loc_mu_1_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_var_1_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_pi_1_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_mu_2_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_var_2_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_pi_2_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_mu_3_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_var_3_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_pi_3_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_mu_4_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_var_4_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        loc_pi_4_layers += [nn.Conv2d(c, number_anchor * 4, kernel_size=3, padding=1)]\n",
    "        conf_mu_1_layers += [nn.Conv2d(c, number_anchor * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_var_1_layers += [nn.Conv2d(c, number_anchor * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_pi_1_layers += [nn.Conv2d(c, number_anchor * 1, kernel_size=3, padding=1)]\n",
    "        conf_mu_2_layers += [nn.Conv2d(c, number_anchor * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_var_2_layers += [nn.Conv2d(c, number_anchor * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_pi_2_layers += [nn.Conv2d(c, number_anchor * 1, kernel_size=3, padding=1)]\n",
    "        conf_mu_3_layers += [nn.Conv2d(c, number_anchor * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_var_3_layers += [nn.Conv2d(c, number_anchor * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_pi_3_layers += [nn.Conv2d(c, number_anchor * 1, kernel_size=3, padding=1)]\n",
    "        conf_mu_4_layers += [nn.Conv2d(c, number_anchor * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_var_4_layers += [nn.Conv2d(c, number_anchor * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_pi_4_layers += [nn.Conv2d(c, number_anchor * 1, kernel_size=3, padding=1)]\n",
    "\n",
    "    return (\n",
    "        loc_mu_1_layers, loc_var_1_layers, loc_pi_1_layers, loc_mu_2_layers, loc_var_2_layers, loc_pi_2_layers, \\\n",
    "        loc_mu_3_layers, loc_var_3_layers, loc_pi_3_layers, loc_mu_4_layers, loc_var_4_layers, loc_pi_4_layers, \\\n",
    "        conf_mu_1_layers, conf_var_1_layers, conf_pi_1_layers, conf_mu_2_layers, conf_var_2_layers, conf_pi_2_layers, \\\n",
    "        conf_mu_3_layers, conf_var_3_layers, conf_pi_3_layers, conf_mu_4_layers, conf_var_4_layers, conf_pi_4_layers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d591fb9-3d67-491f-8a5b-9a02c4697083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09d9cbfe-6032-410c-8c6b-8ebc20c857c1",
   "metadata": {},
   "source": [
    "#### PriorBox, Detect_GMM, And Test condition is remaining\n",
    "that part is commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c03e3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originated from https://github.com/amdegroot/ssd.pytorch\n",
    "\n",
    "from __future__ import division\n",
    "from math import sqrt as sqrt\n",
    "from itertools import product as product\n",
    "import torch\n",
    "\n",
    "\n",
    "class PriorBox(object):\n",
    "    \"\"\"Compute priorbox coordinates in center-offset form for each source\n",
    "    feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(PriorBox, self).__init__()\n",
    "        self.image_size = cfg['min_dim']\n",
    "        # number of priors for feature map location (either 4 or 6)\n",
    "        self.num_priors = len(cfg['aspect_ratios'])\n",
    "        self.variance = cfg['variance'] or [0.1]\n",
    "        self.feature_maps = cfg['feature_maps']\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.max_sizes = cfg['max_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.aspect_ratios = cfg['aspect_ratios']\n",
    "        self.clip = cfg['clip'] \n",
    "        self.version = cfg['name']\n",
    "        for v in self.variance:\n",
    "            if v <= 0:\n",
    "                raise ValueError('Variances must be greater than 0')\n",
    "\n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                f_k = self.image_size / self.steps[k]\n",
    "                # unit center x,y\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: min_size\n",
    "                s_k = self.min_sizes[k]/self.image_size\n",
    "                mean += [cx, cy, s_k, s_k]\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: sqrt(s_k * s_(k+1))\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
    "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "                # rest of aspect ratios\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
    "        # back to torch land\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f3f8efa2-d81e-4779-a3b9-c491a9ce3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class YOLO_GMM(nn.Module):\n",
    "    def __init__(self, gmm_head ,in_channels=3, num_classes=80, phase = \"train\"):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.phase = phase\n",
    "        ## ***********************\n",
    "        # REMAINING\n",
    "        self.priorbox = PriorBox(coco300_active) # '''???'''\n",
    "        \n",
    "        with torch.no_grad():\n",
    "           self.priors = Variable(self.priorbox.forward())\n",
    "        \n",
    "        self.layers = self._create_conv_layers()\n",
    "        # localization GMM parameters\n",
    "        self.loc_mu_1 = nn.ModuleList(gmm_head[0])\n",
    "        self.loc_var_1 = nn.ModuleList(gmm_head[1])\n",
    "        self.loc_pi_1 = nn.ModuleList(gmm_head[2])\n",
    "        self.loc_mu_2 = nn.ModuleList(gmm_head[3])\n",
    "        self.loc_var_2 = nn.ModuleList(gmm_head[4])\n",
    "        self.loc_pi_2 = nn.ModuleList(gmm_head[5])\n",
    "        self.loc_mu_3 = nn.ModuleList(gmm_head[6])\n",
    "        self.loc_var_3 = nn.ModuleList(gmm_head[7])\n",
    "        self.loc_pi_3 = nn.ModuleList(gmm_head[8])\n",
    "        self.loc_mu_4 = nn.ModuleList(gmm_head[9])\n",
    "        self.loc_var_4 = nn.ModuleList(gmm_head[10])\n",
    "        self.loc_pi_4 = nn.ModuleList(gmm_head[11])\n",
    "\n",
    "        # Classification GMM parameters\n",
    "        self.conf_mu_1 = nn.ModuleList(gmm_head[12])\n",
    "        self.conf_var_1 = nn.ModuleList(gmm_head[13])\n",
    "        self.conf_pi_1 = nn.ModuleList(gmm_head[14])\n",
    "        self.conf_mu_2 = nn.ModuleList(gmm_head[15])\n",
    "        self.conf_var_2 = nn.ModuleList(gmm_head[16])\n",
    "        self.conf_pi_2 = nn.ModuleList(gmm_head[17])\n",
    "        self.conf_mu_3 = nn.ModuleList(gmm_head[18])\n",
    "        self.conf_var_3 = nn.ModuleList(gmm_head[19])\n",
    "        self.conf_pi_3 = nn.ModuleList(gmm_head[20])\n",
    "        self.conf_mu_4 = nn.ModuleList(gmm_head[21])\n",
    "        self.conf_var_4 = nn.ModuleList(gmm_head[22])\n",
    "        self.conf_pi_4 = nn.ModuleList(gmm_head[23])\n",
    "        \n",
    "        \n",
    "\n",
    "        # **************\n",
    "        # REMAINING\n",
    "        #if phase == 'test':\n",
    "        #    self.softmax = nn.Softmax(dim=-1)\n",
    "        #    self.detect = Detect_GMM(num_classes, 0, 200, 0.01, 0.45)\n",
    "        \n",
    "        \n",
    "        #self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []  # for each scale\n",
    "        route_connections = []\n",
    "        #sources = list()\n",
    "        loc_mu_1 = list()\n",
    "        loc_var_1 = list()\n",
    "        loc_pi_1 = list()\n",
    "        loc_mu_2 = list()\n",
    "        loc_var_2 = list()\n",
    "        loc_pi_2 = list()\n",
    "        loc_mu_3 = list()\n",
    "        loc_var_3 = list()\n",
    "        loc_pi_3 = list()\n",
    "        loc_mu_4 = list()\n",
    "        loc_var_4 = list()\n",
    "        loc_pi_4 = list()\n",
    "        conf_mu_1 = list()\n",
    "        conf_var_1 = list()\n",
    "        conf_pi_1 = list()\n",
    "        conf_mu_2 = list()\n",
    "        conf_var_2 = list()\n",
    "        conf_pi_2 = list()\n",
    "        conf_mu_3 = list()\n",
    "        conf_var_3 = list()\n",
    "        conf_pi_3 = list()\n",
    "        conf_mu_4 = list()\n",
    "        conf_var_4 = list()\n",
    "        conf_pi_4 = list()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # for layer in self.layers: \n",
    "        # removing ScalePrediction layer and giving previous layer output to gmm layers\n",
    "        for i ,layer in enumerate(self.layers):\n",
    "            #if isinstance(layer, ScalePrediction):\n",
    "            # list represen\n",
    "            \n",
    "            x = layer(x) # moved up to pass previous layer out to next layer\n",
    "            if i in [14, 20, 27]: \n",
    "                outputs.append(x)\n",
    "                \n",
    "            elif isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "               \n",
    "        # for every feature output all corner mu var pi val\n",
    "        for (x, l_mu_1, l_var_1, l_pi_1, l_mu_2, l_var_2, l_pi_2, l_mu_3, l_var_3, l_pi_3, l_mu_4, l_var_4, l_pi_4, \\\n",
    "        c_mu_1, c_var_1, c_pi_1, c_mu_2, c_var_2, c_pi_2, c_mu_3, c_var_3, c_pi_3, c_mu_4, c_var_4, c_pi_4) in zip(outputs, \\\n",
    "        self.loc_mu_1, self.loc_var_1, self.loc_pi_1, self.loc_mu_2, self.loc_var_2, self.loc_pi_2, \\\n",
    "        self.loc_mu_3, self.loc_var_3, self.loc_pi_3, self.loc_mu_4, self.loc_var_4, self.loc_pi_4, \\\n",
    "        self.conf_mu_1, self.conf_var_1, self.conf_pi_1, self.conf_mu_2, self.conf_var_2, self.conf_pi_2, \\\n",
    "        self.conf_mu_3, self.conf_var_3, self.conf_pi_3, self.conf_mu_4, self.conf_var_4, self.conf_pi_4):\n",
    "            \n",
    "            ## no need to changed shape as taking previous layers of scale prid only needed if scaleprid layers are used\n",
    "            ## change shape [2, 3, 52, 52, 85] to [2, 52, 52, 255]\n",
    "            # shape = x.shape\n",
    "            # x = torch.reshape(x.permute(0, 2, 3, 1,4), (shape[0], shape[2],shape[3], shape[1]* shape[4]))\n",
    "            # x = x.permute(0, 3, 1, 2)\n",
    "            \n",
    "            loc_mu_1.append(l_mu_1(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_var_1.append(l_var_1(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_pi_1.append(l_pi_1(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_mu_2.append(l_mu_2(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_var_2.append(l_var_2(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_pi_2.append(l_pi_2(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_mu_3.append(l_mu_3(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_var_3.append(l_var_3(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_pi_3.append(l_pi_3(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_mu_4.append(l_mu_4(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_var_4.append(l_var_4(x).permute(0, 2, 3, 1).contiguous())\n",
    "            loc_pi_4.append(l_pi_4(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_mu_1.append(c_mu_1(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_var_1.append(c_var_1(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_pi_1.append(c_pi_1(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_mu_2.append(c_mu_2(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_var_2.append(c_var_2(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_pi_2.append(c_pi_2(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_mu_3.append(c_mu_3(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_var_3.append(c_var_3(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_pi_3.append(c_pi_3(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_mu_4.append(c_mu_4(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_var_4.append(c_var_4(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf_pi_4.append(c_pi_4(x).permute(0, 2, 3, 1).contiguous())\n",
    "            \n",
    "        # get every think in a row [N, float output of  6 features] \n",
    "        loc_mu_1 = torch.cat([o.view(o.size(0), -1) for o in loc_mu_1], 1)\n",
    "        loc_var_1 = torch.cat([o.view(o.size(0), -1) for o in loc_var_1], 1)\n",
    "        loc_pi_1 = torch.cat([o.view(o.size(0), -1) for o in loc_pi_1], 1)\n",
    "        loc_mu_2 = torch.cat([o.view(o.size(0), -1) for o in loc_mu_2], 1)\n",
    "        loc_var_2 = torch.cat([o.view(o.size(0), -1) for o in loc_var_2], 1)\n",
    "        loc_pi_2 = torch.cat([o.view(o.size(0), -1) for o in loc_pi_2], 1)\n",
    "        loc_mu_3 = torch.cat([o.view(o.size(0), -1) for o in loc_mu_3], 1)\n",
    "        loc_var_3 = torch.cat([o.view(o.size(0), -1) for o in loc_var_3], 1)\n",
    "        loc_pi_3 = torch.cat([o.view(o.size(0), -1) for o in loc_pi_3], 1)\n",
    "        loc_mu_4 = torch.cat([o.view(o.size(0), -1) for o in loc_mu_4], 1)\n",
    "        loc_var_4 = torch.cat([o.view(o.size(0), -1) for o in loc_var_4], 1)\n",
    "        loc_pi_4 = torch.cat([o.view(o.size(0), -1) for o in loc_pi_4], 1)\n",
    "        conf_mu_1 = torch.cat([o.view(o.size(0), -1) for o in conf_mu_1], 1)\n",
    "        conf_var_1 = torch.cat([o.view(o.size(0), -1) for o in conf_var_1], 1)\n",
    "        conf_pi_1 = torch.cat([o.view(o.size(0), -1) for o in conf_pi_1], 1)\n",
    "        conf_mu_2 = torch.cat([o.view(o.size(0), -1) for o in conf_mu_2], 1)\n",
    "        conf_var_2 = torch.cat([o.view(o.size(0), -1) for o in conf_var_2], 1)\n",
    "        conf_pi_2 = torch.cat([o.view(o.size(0), -1) for o in conf_pi_2], 1)\n",
    "        conf_mu_3 = torch.cat([o.view(o.size(0), -1) for o in conf_mu_3], 1)\n",
    "        conf_var_3 = torch.cat([o.view(o.size(0), -1) for o in conf_var_3], 1)\n",
    "        conf_pi_3 = torch.cat([o.view(o.size(0), -1) for o in conf_pi_3], 1)\n",
    "        conf_mu_4 = torch.cat([o.view(o.size(0), -1) for o in conf_mu_4], 1)\n",
    "        conf_var_4 = torch.cat([o.view(o.size(0), -1) for o in conf_var_4], 1)\n",
    "        conf_pi_4 = torch.cat([o.view(o.size(0), -1) for o in conf_pi_4], 1)\n",
    "        \n",
    "        if self.phase == \"test\":\n",
    "            # REMAINING \n",
    "            loc_var_1 = torch.sigmoid(loc_var_1)\n",
    "            loc_var_2 = torch.sigmoid(loc_var_2)\n",
    "            loc_var_3 = torch.sigmoid(loc_var_3)\n",
    "            loc_var_4 = torch.sigmoid(loc_var_4)\n",
    "\n",
    "            loc_pi_1 = loc_pi_1.view(-1, 4)\n",
    "            loc_pi_2 = loc_pi_2.view(-1, 4)\n",
    "            loc_pi_3 = loc_pi_3.view(-1, 4)\n",
    "            loc_pi_4 = loc_pi_4.view(-1, 4)\n",
    "\n",
    "            pi_all = torch.stack(\n",
    "                [\n",
    "                    loc_pi_1.reshape(-1),\n",
    "                    loc_pi_2.reshape(-1),\n",
    "                    loc_pi_3.reshape(-1),\n",
    "                    loc_pi_4.reshape(-1)\n",
    "                ]\n",
    "            )\n",
    "            pi_all = pi_all.transpose(0,1)\n",
    "            pi_all = (torch.softmax(pi_all, dim=1)).transpose(0,1).reshape(-1)\n",
    "            (\n",
    "                loc_pi_1,\n",
    "                loc_pi_2,\n",
    "                loc_pi_3,\n",
    "                loc_pi_4\n",
    "            ) = torch.split(pi_all, loc_pi_1.reshape(-1).size(0), dim=0)\n",
    "            loc_pi_1 = loc_pi_1.view(-1, 4)\n",
    "            loc_pi_2 = loc_pi_2.view(-1, 4)\n",
    "            loc_pi_3 = loc_pi_3.view(-1, 4)\n",
    "            loc_pi_4 = loc_pi_4.view(-1, 4)\n",
    "\n",
    "            conf_var_1 = torch.sigmoid(conf_var_1)\n",
    "            conf_var_2 = torch.sigmoid(conf_var_2)\n",
    "            conf_var_3 = torch.sigmoid(conf_var_3)\n",
    "            conf_var_4 = torch.sigmoid(conf_var_4)\n",
    "\n",
    "            conf_pi_1 = conf_pi_1.view(-1, 1)\n",
    "            conf_pi_2 = conf_pi_2.view(-1, 1)\n",
    "            conf_pi_3 = conf_pi_3.view(-1, 1)\n",
    "            conf_pi_4 = conf_pi_4.view(-1, 1)\n",
    "\n",
    "            conf_pi_all = torch.stack(\n",
    "                [\n",
    "                    conf_pi_1.reshape(-1),\n",
    "                    conf_pi_2.reshape(-1),\n",
    "                    conf_pi_3.reshape(-1),\n",
    "                    conf_pi_4.reshape(-1)\n",
    "                ]\n",
    "            )\n",
    "            conf_pi_all = conf_pi_all.transpose(0,1)\n",
    "            conf_pi_all = (torch.softmax(conf_pi_all, dim=1)).transpose(0,1).reshape(-1)\n",
    "            (\n",
    "                conf_pi_1,\n",
    "                conf_pi_2,\n",
    "                conf_pi_3,\n",
    "                conf_pi_4\n",
    "            ) = torch.split(conf_pi_all, conf_pi_1.reshape(-1).size(0), dim=0)\n",
    "            conf_pi_1 = conf_pi_1.view(-1, 1)\n",
    "            conf_pi_2 = conf_pi_2.view(-1, 1)\n",
    "            conf_pi_3 = conf_pi_3.view(-1, 1)\n",
    "            conf_pi_4 = conf_pi_4.view(-1, 1)\n",
    "\n",
    "            output = self.detect(\n",
    "                self.priors.type(type(x.data)),\n",
    "                loc_mu_1.view(loc_mu_1.size(0), -1, 4),\n",
    "                loc_var_1.view(loc_var_1.size(0), -1, 4),\n",
    "                loc_pi_1.view(loc_var_1.size(0), -1, 4),\n",
    "                loc_mu_2.view(loc_mu_2.size(0), -1, 4),\n",
    "                loc_var_2.view(loc_var_2.size(0), -1, 4),\n",
    "                loc_pi_2.view(loc_var_2.size(0), -1, 4),\n",
    "                loc_mu_3.view(loc_mu_3.size(0), -1, 4),\n",
    "                loc_var_3.view(loc_var_3.size(0), -1, 4),\n",
    "                loc_pi_3.view(loc_var_3.size(0), -1, 4),\n",
    "                loc_mu_4.view(loc_mu_4.size(0), -1, 4),\n",
    "                loc_var_4.view(loc_var_4.size(0), -1, 4),\n",
    "                loc_pi_4.view(loc_var_4.size(0), -1, 4),\n",
    "                self.softmax(conf_mu_1.view(conf_mu_1.size(0), -1, self.num_classes)),\n",
    "                conf_var_1.view(conf_var_1.size(0), -1, self.num_classes),\n",
    "                conf_pi_1.view(conf_var_1.size(0), -1, 1),\n",
    "                self.softmax(conf_mu_2.view(conf_mu_2.size(0), -1, self.num_classes)),\n",
    "                conf_var_2.view(conf_var_2.size(0), -1, self.num_classes),\n",
    "                conf_pi_2.view(conf_var_2.size(0), -1, 1),\n",
    "                self.softmax(conf_mu_3.view(conf_mu_3.size(0), -1, self.num_classes)),\n",
    "                conf_var_3.view(conf_var_3.size(0), -1, self.num_classes),\n",
    "                conf_pi_3.view(conf_var_3.size(0), -1, 1),\n",
    "                self.softmax(conf_mu_4.view(conf_mu_4.size(0), -1, self.num_classes)),\n",
    "                conf_var_4.view(conf_var_4.size(0), -1, self.num_classes),\n",
    "                conf_pi_4.view(conf_var_4.size(0), -1, 1)\n",
    "            )\n",
    "            # pass\n",
    "        else:\n",
    "            gmm_output = (\n",
    "                self.priors,\n",
    "                loc_mu_1.view(loc_mu_1.size(0), -1, 4),\n",
    "                loc_var_1.view(loc_var_1.size(0), -1, 4),\n",
    "                loc_pi_1.view(loc_pi_1.size(0), -1, 4),\n",
    "                loc_mu_2.view(loc_mu_2.size(0), -1, 4),\n",
    "                loc_var_2.view(loc_var_2.size(0), -1, 4),\n",
    "                loc_pi_2.view(loc_pi_2.size(0), -1, 4),\n",
    "                loc_mu_3.view(loc_mu_3.size(0), -1, 4),\n",
    "                loc_var_3.view(loc_var_3.size(0), -1, 4),\n",
    "                loc_pi_3.view(loc_pi_3.size(0), -1, 4),\n",
    "                loc_mu_4.view(loc_mu_4.size(0), -1, 4),\n",
    "                loc_var_4.view(loc_var_4.size(0), -1, 4),\n",
    "                loc_pi_4.view(loc_pi_4.size(0), -1, 4),\n",
    "                conf_mu_1.view(conf_mu_1.size(0), -1, self.num_classes),\n",
    "                conf_var_1.view(conf_var_1.size(0), -1, self.num_classes),\n",
    "                conf_pi_1.view(conf_pi_1.size(0), -1, 1),\n",
    "                conf_mu_2.view(conf_mu_2.size(0), -1, self.num_classes),\n",
    "                conf_var_2.view(conf_var_2.size(0), -1, self.num_classes),\n",
    "                conf_pi_2.view(conf_pi_2.size(0), -1, 1),\n",
    "                conf_mu_3.view(conf_mu_3.size(0), -1, self.num_classes),\n",
    "                conf_var_3.view(conf_var_3.size(0), -1, self.num_classes),\n",
    "                conf_pi_3.view(conf_pi_3.size(0), -1, 1),\n",
    "                conf_mu_4.view(conf_mu_4.size(0), -1, self.num_classes),\n",
    "                conf_var_4.view(conf_var_4.size(0), -1, self.num_classes),\n",
    "                conf_pi_4.view(conf_pi_4.size(0), -1, 1)\n",
    "            )\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "        return gmm_output\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in yolo_config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        #ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2),)\n",
    "                    in_channels = in_channels * 3\n",
    "                    \n",
    "\n",
    "        return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2d554-2126-4be6-887d-fcc17dddec94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "323be050-5f2f-49e5-9305-07cd95bd2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_yolo_gmm(phase = \"train\", size=416, num_classes=80):\n",
    "    if phase != \"test\" and phase != \"train\":\n",
    "        print(\"ERROR: Phase: \" + phase + \" not recognized\")\n",
    "        return\n",
    "    \n",
    "    out_channels = [512, 256, 128] # for before scale prid layer \n",
    "    # out_channels = [255, 255, 255] # if final yolo out given to gmm\n",
    "\n",
    "    gmm_head = multibox(out_channels, num_classes)\n",
    "\n",
    "    return YOLO_GMM(gmm_head , phase = phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b0dbae70-33d9-42c1-a00d-08492b3e9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_yolo_gmm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d0d57592-060a-4f78-ab66-855a72ffe039",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((2, 3, 416, 416))\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "68173830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT of gmm\n",
      "torch.Size([8732, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 4])\n",
      "torch.Size([2, 2535, 80])\n",
      "torch.Size([2, 2535, 80])\n",
      "torch.Size([2, 2535, 1])\n",
      "torch.Size([2, 2535, 80])\n",
      "torch.Size([2, 2535, 80])\n",
      "torch.Size([2, 2535, 1])\n",
      "torch.Size([2, 2535, 80])\n",
      "torch.Size([2, 2535, 80])\n",
      "torch.Size([2, 2535, 1])\n",
      "torch.Size([2, 2535, 80])\n",
      "torch.Size([2, 2535, 80])\n",
      "torch.Size([2, 2535, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"OUTPUT of gmm\")\n",
    "for o in out:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c0d05b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def point_form(boxes):\n",
    "    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n",
    "    representation for comparison to point form ground truth data.\n",
    "    Args:\n",
    "        boxes: (tensor) center-size default boxes from priorbox layers.\n",
    "    Return:\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n",
    "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n",
    "\n",
    "\n",
    "def center_size(boxes):\n",
    "    \"\"\" Convert prior_boxes to (cx, cy, w, h)\n",
    "    representation for comparison to center-size form ground truth data.\n",
    "    Args:\n",
    "        boxes: (tensor) point_form boxes\n",
    "    Return:\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n",
    "                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n",
    "\n",
    "\n",
    "def intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = box_a.size(0)\n",
    "    B = box_b.size(0)\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "\n",
    "def jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = intersect(box_a, box_b)\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
    "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
    "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n",
    "\n",
    "\n",
    "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n",
    "    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n",
    "    overlap, encode the bounding boxes, then return the matched indices\n",
    "    corresponding to both confidence and location preds.\n",
    "    Args:\n",
    "        threshold: (float) The overlap threshold used when mathing boxes.\n",
    "        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n",
    "        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n",
    "        variances: (tensor) Variances corresponding to each prior coord,\n",
    "            Shape: [num_priors, 4].\n",
    "        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n",
    "        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n",
    "        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n",
    "        idx: (int) current batch index\n",
    "    Return:\n",
    "        The matched indices corresponding to 1)location and 2)confidence preds.\n",
    "    \"\"\"\n",
    "    # jaccard index\n",
    "    overlaps = jaccard(\n",
    "        truths,\n",
    "        point_form(priors)\n",
    "    )\n",
    "    # (Bipartite Matching)\n",
    "    # [1,num_objects] best prior for each ground truth\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
    "    # [1,num_priors] best ground truth for each prior\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
    "    best_truth_idx.squeeze_(0)\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n",
    "    # TODO refactor: index  best_prior_idx with long tensor\n",
    "    # ensure every gt matches with its prior of max overlap\n",
    "    for j in range(best_prior_idx.size(0)):\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n",
    "    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n",
    "    conf[best_truth_overlap < threshold] = 0  # label as background\n",
    "    loc = encode(matches, priors, variances)\n",
    "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
    "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
    "\n",
    "\n",
    "def encode(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
    "    we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = torch.log(g_wh + 1e-9) / variances[1]\n",
    "    # return target for smooth_l1_loss\n",
    "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4\n",
    "\n",
    "\n",
    "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
    "def decode(loc, priors, variances):\n",
    "    \"\"\"Decode locations from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        loc (tensor): location predictions for loc layers,\n",
    "            Shape: [num_priors,4]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded bounding box predictions\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    \"\"\"Utility function for computing log_sum_exp while determining\n",
    "    This will be used to determine unaveraged confidence loss across\n",
    "    all examples in a batch.\n",
    "    Args:\n",
    "        x (Variable(tensor)): conf_preds from conf layers\n",
    "    \"\"\"\n",
    "    x_max = x.data.max()\n",
    "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n",
    "\n",
    "\n",
    "# Original author: Francisco Massa:\n",
    "# https://github.com/fmassa/object-detection.torch\n",
    "# Ported to PyTorch by Max deGroot (02/01/2017)\n",
    "def nms(boxes, scores, overlap=0.5, top_k=200):\n",
    "    \"\"\"Apply non-maximum suppression at test time to avoid detecting too many\n",
    "    overlapping bounding boxes for a given object.\n",
    "    Args:\n",
    "        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n",
    "        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n",
    "        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n",
    "        top_k: (int) The Maximum number of box preds to consider.\n",
    "    Return:\n",
    "        The indices of the kept boxes with respect to num_priors.\n",
    "    \"\"\"\n",
    "\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0:\n",
    "        return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    # I = I[v >= 0.01]\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    # keep = torch.Tensor()\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        # keep.append(i)\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1:\n",
    "            break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ae8e4de6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34640\\2283275666.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoco\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# from ..box_utils import match, log_sum_exp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from data import coco as cfg\n",
    "# from ..box_utils import match, log_sum_exp\n",
    "import math\n",
    "\n",
    "def Gaussian(y, mu, var):\n",
    "    eps = 0.3\n",
    "    result = (y-mu)/var\n",
    "    result = (result**2)/2*(-1)\n",
    "    exp = torch.exp(result)\n",
    "    result = exp/(math.sqrt(2*math.pi))/(var + eps)\n",
    "\n",
    "    return result\n",
    "\n",
    "def NLL_loss(bbox_gt, bbox_pred, bbox_var):\n",
    "        bbox_var = torch.sigmoid(bbox_var)\n",
    "        prob = Gaussian(bbox_gt, bbox_pred, bbox_var)\n",
    "\n",
    "        return prob\n",
    "\n",
    "class MultiBoxLoss_GMM(nn.Module):\n",
    "    \"\"\"SSD Weighted Loss Function\n",
    "    Compute Targets:\n",
    "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
    "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
    "           (default threshold: 0.5).\n",
    "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
    "           truth boxes and their matched  'priorboxes'.\n",
    "        3) Hard negative mining to filter the excessive number of negative examples\n",
    "           that comes with using a large number of default bounding boxes.\n",
    "           (default negative:positive ratio 3:1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n",
    "                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n",
    "                 use_gpu=True, cls_type='Type-1'):\n",
    "        super(MultiBoxLoss_GMM, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = overlap_thresh\n",
    "        self.background_label = bkg_label\n",
    "        self.encode_target = encode_target\n",
    "        self.use_prior_for_matching = prior_for_matching\n",
    "        self.do_neg_mining = neg_mining\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.neg_overlap = neg_overlap\n",
    "        # self.variance = cfg['variance']\n",
    "        self.variance = 0.1\n",
    "        self.cls_type = cls_type\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        priors, loc_mu_1, loc_var_1, loc_pi_1, loc_mu_2, loc_var_2, loc_pi_2, \\\n",
    "        loc_mu_3, loc_var_3, loc_pi_3, loc_mu_4, loc_var_4, loc_pi_4, \\\n",
    "        conf_mu_1, conf_var_1, conf_pi_1, conf_mu_2, conf_var_2, conf_pi_2, \\\n",
    "        conf_mu_3, conf_var_3, conf_pi_3, conf_mu_4, conf_var_4, conf_pi_4 = predictions\n",
    "\n",
    "        num = loc_mu_1.size(0)\n",
    "        priors = priors[:loc_mu_1.size(1), :]\n",
    "        num_priors = (priors.size(0))\n",
    "        num_classes = self.num_classes\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :-1].data\n",
    "            labels = targets[idx][:, -1].data\n",
    "            defaults = priors.data\n",
    "            match(self.threshold,\n",
    "                  truths,\n",
    "                  defaults,\n",
    "                  self.variance,\n",
    "                  labels,\n",
    "                  loc_t,\n",
    "                  conf_t,\n",
    "                  idx)\n",
    "        if self.use_gpu:\n",
    "            loc_t = loc_t.cuda()\n",
    "            conf_t = conf_t.cuda()\n",
    "        # wrap targets\n",
    "        loc_t = Variable(loc_t, requires_grad=False)\n",
    "        conf_t = Variable(conf_t, requires_grad=False)\n",
    "\n",
    "        pos = conf_t > 0\n",
    "        num_pos = pos.sum(dim=1, keepdim=True)\n",
    "\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_mu_1)\n",
    "        loc_mu_1_ = loc_mu_1[pos_idx].view(-1, 4)\n",
    "        loc_mu_2_ = loc_mu_2[pos_idx].view(-1, 4)\n",
    "        loc_mu_3_ = loc_mu_3[pos_idx].view(-1, 4)\n",
    "        loc_mu_4_ = loc_mu_4[pos_idx].view(-1, 4)\n",
    "\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "\n",
    "        # localization loss\n",
    "        loss_l_1 = NLL_loss(loc_t, loc_mu_1_, loc_var_1[pos_idx].view(-1, 4))\n",
    "        loss_l_2 = NLL_loss(loc_t, loc_mu_2_, loc_var_2[pos_idx].view(-1, 4))\n",
    "        loss_l_3 = NLL_loss(loc_t, loc_mu_3_, loc_var_3[pos_idx].view(-1, 4))\n",
    "        loss_l_4 = NLL_loss(loc_t, loc_mu_4_, loc_var_4[pos_idx].view(-1, 4))\n",
    "\n",
    "        loc_pi_1_ = loc_pi_1[pos_idx].view(-1, 4)\n",
    "        loc_pi_2_ = loc_pi_2[pos_idx].view(-1, 4)\n",
    "        loc_pi_3_ = loc_pi_3[pos_idx].view(-1, 4)\n",
    "        loc_pi_4_ = loc_pi_4[pos_idx].view(-1, 4)\n",
    "\n",
    "        pi_all = torch.stack([\n",
    "                    loc_pi_1_.reshape(-1),\n",
    "                    loc_pi_2_.reshape(-1),\n",
    "                    loc_pi_3_.reshape(-1),\n",
    "                    loc_pi_4_.reshape(-1)\n",
    "                    ])\n",
    "        pi_all = pi_all.transpose(0,1)\n",
    "        pi_all = (torch.softmax(pi_all, dim=1)).transpose(0,1).reshape(-1)\n",
    "        (\n",
    "            loc_pi_1_,\n",
    "            loc_pi_2_,\n",
    "            loc_pi_3_,\n",
    "            loc_pi_4_\n",
    "        ) = torch.split(pi_all, loc_pi_1_.reshape(-1).size(0), dim=0)\n",
    "        loc_pi_1_ = loc_pi_1_.view(-1, 4)\n",
    "        loc_pi_2_ = loc_pi_2_.view(-1, 4)\n",
    "        loc_pi_3_ = loc_pi_3_.view(-1, 4)\n",
    "        loc_pi_4_ = loc_pi_4_.view(-1, 4)\n",
    "\n",
    "        _loss_l = (\n",
    "            loc_pi_1_*loss_l_1 +\n",
    "            loc_pi_2_*loss_l_2 +\n",
    "            loc_pi_3_*loss_l_3 +\n",
    "            loc_pi_4_*loss_l_4\n",
    "        )\n",
    "\n",
    "        epsi = 10**-9\n",
    "        # balance parameter\n",
    "        balance = 2.0\n",
    "        loss_l = -torch.log(_loss_l + epsi)/balance\n",
    "        loss_l = loss_l.sum()\n",
    "\n",
    "        if self.cls_type == 'Type-1':\n",
    "            # Classification loss (Type-1)\n",
    "            conf_pi_1_ = conf_pi_1.view(-1, 1)\n",
    "            conf_pi_2_ = conf_pi_2.view(-1, 1)\n",
    "            conf_pi_3_ = conf_pi_3.view(-1, 1)\n",
    "            conf_pi_4_ = conf_pi_4.view(-1, 1)\n",
    "\n",
    "            conf_pi_all = torch.stack([\n",
    "                            conf_pi_1_.reshape(-1),\n",
    "                            conf_pi_2_.reshape(-1),\n",
    "                            conf_pi_3_.reshape(-1),\n",
    "                            conf_pi_4_.reshape(-1)\n",
    "                            ])\n",
    "            conf_pi_all = conf_pi_all.transpose(0,1)\n",
    "            conf_pi_all = (torch.softmax(conf_pi_all, dim=1)).transpose(0,1).reshape(-1)\n",
    "            (\n",
    "                conf_pi_1_,\n",
    "                conf_pi_2_,\n",
    "                conf_pi_3_,\n",
    "                conf_pi_4_\n",
    "            ) = torch.split(conf_pi_all, conf_pi_1_.reshape(-1).size(0), dim=0)\n",
    "            conf_pi_1_ = conf_pi_1_.view(conf_pi_1.size(0), -1)\n",
    "            conf_pi_2_ = conf_pi_2_.view(conf_pi_2.size(0), -1)\n",
    "            conf_pi_3_ = conf_pi_3_.view(conf_pi_3.size(0), -1)\n",
    "            conf_pi_4_ = conf_pi_4_.view(conf_pi_4.size(0), -1)\n",
    "\n",
    "            conf_var_1 = torch.sigmoid(conf_var_1)\n",
    "            conf_var_2 = torch.sigmoid(conf_var_2)\n",
    "            conf_var_3 = torch.sigmoid(conf_var_3)\n",
    "            conf_var_4 = torch.sigmoid(conf_var_4)\n",
    "\n",
    "            rand_val_1 = torch.randn(conf_var_1.size(0), conf_var_1.size(1), conf_var_1.size(2))\n",
    "            rand_val_2 = torch.randn(conf_var_2.size(0), conf_var_2.size(1), conf_var_2.size(2))\n",
    "            rand_val_3 = torch.randn(conf_var_3.size(0), conf_var_3.size(1), conf_var_3.size(2))\n",
    "            rand_val_4 = torch.randn(conf_var_4.size(0), conf_var_4.size(1), conf_var_4.size(2))\n",
    "\n",
    "            batch_conf_1 = (conf_mu_1+torch.sqrt(conf_var_1)*rand_val_1).view(-1, self.num_classes)\n",
    "            batch_conf_2 = (conf_mu_2+torch.sqrt(conf_var_2)*rand_val_2).view(-1, self.num_classes)\n",
    "            batch_conf_3 = (conf_mu_3+torch.sqrt(conf_var_3)*rand_val_3).view(-1, self.num_classes)\n",
    "            batch_conf_4 = (conf_mu_4+torch.sqrt(conf_var_4)*rand_val_4).view(-1, self.num_classes)\n",
    "\n",
    "            loss_c_1 = log_sum_exp(batch_conf_1) - batch_conf_1.gather(1, conf_t.view(-1, 1))\n",
    "            loss_c_2 = log_sum_exp(batch_conf_2) - batch_conf_2.gather(1, conf_t.view(-1, 1))\n",
    "            loss_c_3 = log_sum_exp(batch_conf_3) - batch_conf_3.gather(1, conf_t.view(-1, 1))\n",
    "            loss_c_4 = log_sum_exp(batch_conf_4) - batch_conf_4.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "            loss_c = (\n",
    "                loss_c_1 * conf_pi_1_.view(-1, 1) +\n",
    "                loss_c_2 * conf_pi_2_.view(-1, 1) +\n",
    "                loss_c_3 * conf_pi_3_.view(-1, 1) +\n",
    "                loss_c_4 * conf_pi_4_.view(-1, 1)\n",
    "            )\n",
    "            loss_c = loss_c.view(pos.size()[0], pos.size()[1])\n",
    "            loss_c[pos] = 0  # filter out pos boxes for now  : true -> zero\n",
    "            loss_c = loss_c.view(num, -1)\n",
    "\n",
    "            _, loss_idx = loss_c.sort(1, descending=True)\n",
    "            _, idx_rank = loss_idx.sort(1)\n",
    "            num_pos = pos.long().sum(1, keepdim=True)\n",
    "            num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "            neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "            # Confidence Loss Including Positive and Negative Examples\n",
    "            pos_idx = pos.unsqueeze(2).expand_as(conf_mu_1)\n",
    "            neg_idx = neg.unsqueeze(2).expand_as(conf_mu_1)\n",
    "\n",
    "            batch_conf_1_ = conf_mu_1+torch.sqrt(conf_var_1)*rand_val_1\n",
    "            batch_conf_2_ = conf_mu_2+torch.sqrt(conf_var_2)*rand_val_2\n",
    "            batch_conf_3_ = conf_mu_3+torch.sqrt(conf_var_3)*rand_val_3\n",
    "            batch_conf_4_ = conf_mu_4+torch.sqrt(conf_var_4)*rand_val_4\n",
    "\n",
    "            conf_pred_1 = batch_conf_1_[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "            conf_pred_2 = batch_conf_2_[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "            conf_pred_3 = batch_conf_3_[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "            conf_pred_4 = batch_conf_4_[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "\n",
    "            targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "\n",
    "            loss_c_1 = log_sum_exp(conf_pred_1) - conf_pred_1.gather(1, targets_weighted.view(-1, 1))\n",
    "            loss_c_2 = log_sum_exp(conf_pred_2) - conf_pred_2.gather(1, targets_weighted.view(-1, 1))\n",
    "            loss_c_3 = log_sum_exp(conf_pred_3) - conf_pred_3.gather(1, targets_weighted.view(-1, 1))\n",
    "            loss_c_4 = log_sum_exp(conf_pred_4) - conf_pred_4.gather(1, targets_weighted.view(-1, 1))\n",
    "\n",
    "            _conf_pi_1 = conf_pi_1_[(pos+neg).gt(0)]\n",
    "            _conf_pi_2 = conf_pi_2_[(pos+neg).gt(0)]\n",
    "            _conf_pi_3 = conf_pi_3_[(pos+neg).gt(0)]\n",
    "            _conf_pi_4 = conf_pi_4_[(pos+neg).gt(0)]\n",
    "\n",
    "            loss_c = (\n",
    "                loss_c_1 * _conf_pi_1.view(-1, 1) +\n",
    "                loss_c_2 * _conf_pi_2.view(-1, 1) +\n",
    "                loss_c_3 * _conf_pi_3.view(-1, 1) +\n",
    "                loss_c_4 * _conf_pi_4.view(-1, 1)\n",
    "            )\n",
    "            loss_c = loss_c.sum()\n",
    "\n",
    "        else:\n",
    "            # Classification loss (Type-2)\n",
    "            # more details are in our supplementary material\n",
    "            conf_pi_1_ = conf_pi_1.view(-1, 1)\n",
    "            conf_pi_2_ = conf_pi_2.view(-1, 1)\n",
    "            conf_pi_3_ = conf_pi_3.view(-1, 1)\n",
    "            conf_pi_4_ = conf_pi_4.view(-1, 1)\n",
    "\n",
    "            conf_pi_all = torch.stack([\n",
    "                            conf_pi_1_.reshape(-1),\n",
    "                            conf_pi_2_.reshape(-1),\n",
    "                            conf_pi_3_.reshape(-1),\n",
    "                            conf_pi_4_.reshape(-1)\n",
    "                            ])\n",
    "            conf_pi_all = conf_pi_all.transpose(0,1)\n",
    "            conf_pi_all = (torch.softmax(conf_pi_all, dim=1)).transpose(0,1).reshape(-1)\n",
    "            (\n",
    "                conf_pi_1_,\n",
    "                conf_pi_2_,\n",
    "                conf_pi_3_,\n",
    "                conf_pi_4_\n",
    "            ) = torch.split(conf_pi_all, conf_pi_1_.reshape(-1).size(0), dim=0)\n",
    "            conf_pi_1_ = conf_pi_1_.view(conf_pi_1.size(0), -1)\n",
    "            conf_pi_2_ = conf_pi_2_.view(conf_pi_2.size(0), -1)\n",
    "            conf_pi_3_ = conf_pi_3_.view(conf_pi_3.size(0), -1)\n",
    "            conf_pi_4_ = conf_pi_4_.view(conf_pi_4.size(0), -1)\n",
    "\n",
    "            conf_var_1 = torch.sigmoid(conf_var_1)\n",
    "            conf_var_2 = torch.sigmoid(conf_var_2)\n",
    "            conf_var_3 = torch.sigmoid(conf_var_3)\n",
    "            conf_var_4 = torch.sigmoid(conf_var_4)\n",
    "\n",
    "            rand_val_1 = torch.randn(conf_var_1.size(0), conf_var_1.size(1), conf_var_1.size(2))\n",
    "            rand_val_2 = torch.randn(conf_var_2.size(0), conf_var_2.size(1), conf_var_2.size(2))\n",
    "            rand_val_3 = torch.randn(conf_var_3.size(0), conf_var_3.size(1), conf_var_3.size(2))\n",
    "            rand_val_4 = torch.randn(conf_var_4.size(0), conf_var_4.size(1), conf_var_4.size(2))\n",
    "\n",
    "            batch_conf_1 = (conf_mu_1+torch.sqrt(conf_var_1)*rand_val_1).view(-1, self.num_classes)\n",
    "            batch_conf_2 = (conf_mu_2+torch.sqrt(conf_var_2)*rand_val_2).view(-1, self.num_classes)\n",
    "            batch_conf_3 = (conf_mu_3+torch.sqrt(conf_var_3)*rand_val_3).view(-1, self.num_classes)\n",
    "            batch_conf_4 = (conf_mu_4+torch.sqrt(conf_var_4)*rand_val_4).view(-1, self.num_classes)\n",
    "\n",
    "            soft_max = nn.Softmax(dim=1)\n",
    "\n",
    "            epsi = 10**-9\n",
    "            weighted_softmax_out = (\n",
    "                        soft_max(batch_conf_1)*conf_pi_1_.view(-1, 1) +\n",
    "                        soft_max(batch_conf_2)*conf_pi_2_.view(-1, 1) +\n",
    "                        soft_max(batch_conf_3)*conf_pi_3_.view(-1, 1) +\n",
    "                        soft_max(batch_conf_4)*conf_pi_4_.view(-1, 1)\n",
    "            )\n",
    "            softmax_out_log = -torch.log(weighted_softmax_out+epsi)\n",
    "            loss_c = softmax_out_log.gather(1, conf_t.view(-1,1))\n",
    "\n",
    "            loss_c = loss_c.view(pos.size()[0], pos.size()[1])\n",
    "            loss_c[pos] = 0  # filter out pos boxes for now  : true -> zero\n",
    "            loss_c = loss_c.view(num, -1)\n",
    "\n",
    "            _, loss_idx = loss_c.sort(1, descending=True)\n",
    "            _, idx_rank = loss_idx.sort(1)\n",
    "            num_pos = pos.long().sum(1, keepdim=True)\n",
    "            num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "            neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "            # Confidence Loss Including Positive and Negative Examples\n",
    "            pos_idx = pos.unsqueeze(2).expand_as(conf_mu_1)\n",
    "            neg_idx = neg.unsqueeze(2).expand_as(conf_mu_1)\n",
    "\n",
    "            batch_conf_1_ = conf_mu_1+torch.sqrt(conf_var_1)*rand_val_1\n",
    "            batch_conf_2_ = conf_mu_2+torch.sqrt(conf_var_2)*rand_val_2\n",
    "            batch_conf_3_ = conf_mu_3+torch.sqrt(conf_var_3)*rand_val_3\n",
    "            batch_conf_4_ = conf_mu_4+torch.sqrt(conf_var_4)*rand_val_4\n",
    "\n",
    "            conf_pred_1 = batch_conf_1_[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "            conf_pred_2 = batch_conf_2_[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "            conf_pred_3 = batch_conf_3_[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "            conf_pred_4 = batch_conf_4_[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "\n",
    "            targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "\n",
    "            _conf_pi_1 = conf_pi_1_[(pos+neg).gt(0)]\n",
    "            _conf_pi_2 = conf_pi_2_[(pos+neg).gt(0)]\n",
    "            _conf_pi_3 = conf_pi_3_[(pos+neg).gt(0)]\n",
    "            _conf_pi_4 = conf_pi_4_[(pos+neg).gt(0)]\n",
    "\n",
    "            weighted_softmax_out = (\n",
    "                        soft_max(conf_pred_1)*_conf_pi_1.view(-1, 1) +\n",
    "                        soft_max(conf_pred_2)*_conf_pi_2.view(-1, 1) +\n",
    "                        soft_max(conf_pred_3)*_conf_pi_3.view(-1, 1) +\n",
    "                        soft_max(conf_pred_4)*_conf_pi_4.view(-1, 1)\n",
    "            )\n",
    "            softmax_out_log = -torch.log(weighted_softmax_out+epsi)\n",
    "            loss_c = softmax_out_log.gather(1, targets_weighted.view(-1,1))\n",
    "            loss_c = loss_c.sum()\n",
    "\n",
    "        N = num_pos.data.sum()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        return loss_l, loss_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37488a-ebf2-4995-95ee-c93aa25a319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2069 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34640\\2805824715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34640\\2805824715.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m#plot_couple_examples(model, test_loader, 0.6, 0.5, scaled_anchors)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_anchors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m#if config.SAVE_MODEL:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34640\\2805824715.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_anchors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_anchors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[1;33m+\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_anchors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             )\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlcv_project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Sem 10\\CS776A Deep Learning For Computer Vision\\Yolov3\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, predictions, target, anchors)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         no_object_loss = self.bce(\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnoobj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnoobj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         )\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main file for training Yolo model on Pascal VOC and COCO dataset\n",
    "\"\"\"\n",
    "\n",
    "import config\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from model import YOLOv3\n",
    "from tqdm import tqdm\n",
    "from utils import (\n",
    "    mean_average_precision,\n",
    "    cells_to_bboxes,\n",
    "    get_evaluation_bboxes,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    check_class_accuracy,\n",
    "    get_loaders,\n",
    "    plot_couple_examples\n",
    ")\n",
    "from loss import YoloLoss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    losses = []\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(config.DEVICE)\n",
    "        y0, y1, y2 = (\n",
    "            y[0].to(config.DEVICE),\n",
    "            y[1].to(config.DEVICE),\n",
    "            y[2].to(config.DEVICE),\n",
    "        )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(x)\n",
    "            loss = (\n",
    "                loss_fn(out[0], y0, scaled_anchors[0])\n",
    "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
    "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
    "            )\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update progress bar\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        loop.set_postfix(loss=mean_loss)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # model = YOLO_GMM(num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
    "    model = build_yolo_gmm().to(config.DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_loader, test_loader, train_eval_loader = get_loaders(\n",
    "        train_csv_path=config.DATASET + \"/train.csv\", test_csv_path=config.DATASET + \"/test.csv\"\n",
    "    )\n",
    "\n",
    "    # if config.LOAD_MODEL:\n",
    "    #     load_checkpoint(\n",
    "    #         config.CHECKPOINT_FILE, model, optimizer, config.LEARNING_RATE\n",
    "    #     )\n",
    "\n",
    "    scaled_anchors = (\n",
    "        torch.tensor(config.ANCHORS)\n",
    "        * torch.tensor(config.S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    ).to(config.DEVICE)\n",
    "\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        #plot_couple_examples(model, test_loader, 0.6, 0.5, scaled_anchors)\n",
    "        train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
    "\n",
    "        #if config.SAVE_MODEL:\n",
    "        #    save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
    "\n",
    "        #print(f\"Currently epoch {epoch}\")\n",
    "        #print(\"On Train Eval loader:\")\n",
    "        #print(\"On Train loader:\")\n",
    "        #check_class_accuracy(model, train_loader, threshold=config.CONF_THRESHOLD)\n",
    "\n",
    "        if epoch > 0 and epoch % 3 == 0:\n",
    "            check_class_accuracy(model, test_loader, threshold=config.CONF_THRESHOLD)\n",
    "            pred_boxes, true_boxes = get_evaluation_bboxes(\n",
    "                test_loader,\n",
    "                model,\n",
    "                iou_threshold=config.NMS_IOU_THRESH,\n",
    "                anchors=config.ANCHORS,\n",
    "                threshold=config.CONF_THRESHOLD,\n",
    "            )\n",
    "            mapval = mean_average_precision(\n",
    "                pred_boxes,\n",
    "                true_boxes,\n",
    "                iou_threshold=config.MAP_IOU_THRESH,\n",
    "                box_format=\"midpoint\",\n",
    "                num_classes=config.NUM_CLASSES,\n",
    "            )\n",
    "            print(f\"MAP: {mapval.item()}\")\n",
    "            model.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
